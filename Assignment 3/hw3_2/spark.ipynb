{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8235d61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /Users/xxittysnxx/miniconda3/lib/python3.12/site-packages (3.5.5)\n",
      "Requirement already satisfied: graphframes in /Users/xxittysnxx/miniconda3/lib/python3.12/site-packages (0.6)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /Users/xxittysnxx/miniconda3/lib/python3.12/site-packages (from pyspark) (0.10.9.7)\n",
      "Requirement already satisfied: numpy in /Users/xxittysnxx/miniconda3/lib/python3.12/site-packages (from graphframes) (2.2.4)\n",
      "Requirement already satisfied: nose in /Users/xxittysnxx/miniconda3/lib/python3.12/site-packages (from graphframes) (1.3.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspark graphframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6827db12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/23 17:45:44 WARN Utils: Your hostname, carlostsai.local resolves to a loopback address: 127.0.0.1; using 192.168.1.121 instead (on interface en0)\n",
      "25/04/23 17:45:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /Users/xxittysnxx/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/xxittysnxx/.ivy2/jars\n",
      "graphframes#graphframes added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-fc365447-f502-4b50-8885-523713ebf829;1.0\n",
      "\tconfs: [default]\n",
      "\tfound graphframes#graphframes;0.8.2-spark3.1-s_2.12 in spark-packages\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
      ":: resolution report :: resolve 60ms :: artifacts dl 3ms\n",
      "\t:: modules in use:\n",
      "\tgraphframes#graphframes;0.8.2-spark3.1-s_2.12 from spark-packages in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-fc365447-f502-4b50-8885-523713ebf829\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/2ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/xxittysnxx/miniconda3/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/23 17:45:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "/Users/xxittysnxx/miniconda3/lib/python3.12/site-packages/pyspark/sql/dataframe.py:168: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from graphframes import GraphFrame\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Social Network Analysis\") \\\n",
    "    .config(\"spark.jars.packages\", \"graphframes:graphframes:0.8.2-spark3.1-s_2.12\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load SNAP dataset com-Amazon\n",
    "input_path = \"data/com-amazon.all.dedup.cmty.txt\"\n",
    "output_path = \"data/amazon_edges.txt\"\n",
    "\n",
    "with open(input_path, \"r\") as infile, open(output_path, \"w\") as outfile:\n",
    "    for line in infile:\n",
    "        nodes = line.strip().split()\n",
    "        src = nodes[0]\n",
    "        for dst in nodes[1:]:\n",
    "            if src != dst:\n",
    "                outfile.write(f\"{src}\\t{dst}\\n\")\n",
    "\n",
    "edges_df = spark.read.csv(\"amazon_edges.txt\", sep=\"\\t\", inferSchema=True).toDF(\"src\", \"dst\")\n",
    "\n",
    "# Create edges\n",
    "edges_df = edges_df.union(edges_df.select(col(\"dst\").alias(\"src\"), col(\"src\").alias(\"dst\"))).distinct()\n",
    "\n",
    "# Create vertices\n",
    "vertices_df = edges_df.select(col(\"src\").alias(\"id\")).union(edges_df.select(col(\"dst\").alias(\"id\"))).distinct()\n",
    "\n",
    "# Build GraphFrame\n",
    "g = GraphFrame(vertices_df, edges_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d51ba4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xxittysnxx/miniconda3/lib/python3.12/site-packages/pyspark/sql/dataframe.py:147: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n",
      "[Stage 4:>                                                          (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|outDegree|\n",
      "+---+---------+\n",
      "| 21|    62166|\n",
      "|  7|    53550|\n",
      "|  6|    45977|\n",
      "| 11|    40290|\n",
      "|  1|    26966|\n",
      "+---+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# a. Find the top 5 nodes with the highest outdegree and find the count of the number of outgoing edges in each\n",
    "g.outDegrees.orderBy(\"outDegree\", ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cc91cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:======================================>                   (8 + 4) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|inDegree|\n",
      "+---+--------+\n",
      "| 21|   62166|\n",
      "|  7|   53550|\n",
      "|  6|   45977|\n",
      "| 11|   40290|\n",
      "|  1|   26966|\n",
      "+---+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# b. Find the top 5 nodes with the highest indegree and find the count of the number of incoming edges in each\n",
    "g.inDegrees.orderBy(\"inDegree\", ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77a4cfda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/23 17:45:55 WARN MemoryStore: Not enough space to cache rdd_86_4 in memory! (computed 38.3 MiB so far)\n",
      "25/04/23 17:45:55 WARN MemoryStore: Not enough space to cache rdd_82_7 in memory! (computed 44.1 MiB so far)\n",
      "25/04/23 17:45:55 WARN MemoryStore: Not enough space to cache rdd_82_2 in memory! (computed 38.6 MiB so far)\n",
      "25/04/23 17:45:55 WARN BlockManager: Block rdd_86_4 could not be removed as it was not found on disk or in memory\n",
      "25/04/23 17:45:55 WARN BlockManager: Block rdd_82_2 could not be removed as it was not found on disk or in memory\n",
      "25/04/23 17:45:55 WARN BlockManager: Putting block rdd_86_4 failed\n",
      "25/04/23 17:45:55 WARN BlockManager: Block rdd_82_7 could not be removed as it was not found on disk or in memory\n",
      "25/04/23 17:45:55 WARN BlockManager: Putting block rdd_82_7 failed\n",
      "25/04/23 17:45:55 WARN BlockManager: Putting block rdd_82_2 failed\n",
      "25/04/23 17:45:55 WARN MemoryStore: Not enough space to cache rdd_86_2 in memory! (computed 38.6 MiB so far)\n",
      "25/04/23 17:45:55 WARN BlockManager: Block rdd_86_2 could not be removed as it was not found on disk or in memory\n",
      "25/04/23 17:45:55 WARN BlockManager: Putting block rdd_86_2 failed\n",
      "25/04/23 17:45:55 WARN MemoryStore: Not enough space to cache rdd_86_7 in memory! (computed 44.1 MiB so far)\n",
      "25/04/23 17:45:55 WARN BlockManager: Block rdd_86_7 could not be removed as it was not found on disk or in memory\n",
      "25/04/23 17:45:55 WARN BlockManager: Putting block rdd_86_7 failed\n",
      "25/04/23 17:45:55 WARN MemoryStore: Not enough space to cache rdd_86_6 in memory! (computed 39.6 MiB so far)\n",
      "25/04/23 17:45:55 WARN BlockManager: Block rdd_86_6 could not be removed as it was not found on disk or in memory\n",
      "25/04/23 17:45:55 WARN BlockManager: Putting block rdd_86_6 failed\n",
      "25/04/23 17:46:00 WARN MemoryStore: Not enough space to cache rdd_106_0 in memory! (computed 17.9 MiB so far)\n",
      "25/04/23 17:46:00 WARN BlockManager: Block rdd_106_0 could not be removed as it was not found on disk or in memory\n",
      "25/04/23 17:46:00 WARN BlockManager: Putting block rdd_106_0 failed\n",
      "25/04/23 17:46:00 WARN MemoryStore: Not enough space to cache rdd_106_1 in memory! (computed 17.9 MiB so far)\n",
      "25/04/23 17:46:00 WARN BlockManager: Block rdd_106_1 could not be removed as it was not found on disk or in memory\n",
      "25/04/23 17:46:00 WARN BlockManager: Putting block rdd_106_1 failed\n",
      "25/04/23 17:46:00 WARN MemoryStore: Not enough space to cache rdd_82_3 in memory! (computed 39.3 MiB so far)\n",
      "25/04/23 17:46:00 WARN BlockManager: Block rdd_82_3 could not be removed as it was not found on disk or in memory\n",
      "25/04/23 17:46:00 WARN BlockManager: Putting block rdd_82_3 failed\n",
      "25/04/23 17:46:00 WARN MemoryStore: Not enough space to cache rdd_86_3 in memory! (computed 39.3 MiB so far)\n",
      "25/04/23 17:46:00 WARN BlockManager: Block rdd_86_3 could not be removed as it was not found on disk or in memory\n",
      "25/04/23 17:46:00 WARN BlockManager: Putting block rdd_86_3 failed\n",
      "25/04/23 17:46:00 WARN MemoryStore: Not enough space to cache rdd_82_2 in memory! (computed 38.6 MiB so far)\n",
      "25/04/23 17:46:00 WARN BlockManager: Block rdd_82_2 could not be removed as it was not found on disk or in memory\n",
      "25/04/23 17:46:00 WARN BlockManager: Putting block rdd_82_2 failed\n",
      "25/04/23 17:46:00 WARN MemoryStore: Not enough space to cache rdd_86_2 in memory! (computed 38.6 MiB so far)\n",
      "25/04/23 17:46:00 WARN BlockManager: Block rdd_86_2 could not be removed as it was not found on disk or in memory\n",
      "25/04/23 17:46:00 WARN BlockManager: Putting block rdd_86_2 failed\n",
      "25/04/23 17:46:00 WARN MemoryStore: Not enough space to cache rdd_96_2 in memory! (computed 16.0 MiB so far)\n",
      "25/04/23 17:46:00 WARN BlockManager: Block rdd_96_2 could not be removed as it was not found on disk or in memory\n",
      "25/04/23 17:46:00 WARN BlockManager: Putting block rdd_96_2 failed\n",
      "25/04/23 17:46:00 WARN MemoryStore: Not enough space to cache rdd_106_3 in memory! (computed 18.2 MiB so far)\n",
      "25/04/23 17:46:00 WARN BlockManager: Block rdd_106_3 could not be removed as it was not found on disk or in memory\n",
      "25/04/23 17:46:00 WARN BlockManager: Putting block rdd_106_3 failed\n",
      "25/04/23 17:46:00 WARN MemoryStore: Not enough space to cache rdd_82_7 in memory! (computed 44.1 MiB so far)\n",
      "25/04/23 17:46:00 WARN BlockManager: Block rdd_82_7 could not be removed as it was not found on disk or in memory\n",
      "25/04/23 17:46:00 WARN BlockManager: Putting block rdd_82_7 failed\n",
      "25/04/23 17:46:00 WARN MemoryStore: Not enough space to cache rdd_86_7 in memory! (computed 44.1 MiB so far)\n",
      "25/04/23 17:46:00 WARN BlockManager: Block rdd_86_7 could not be removed as it was not found on disk or in memory\n",
      "25/04/23 17:46:00 WARN BlockManager: Putting block rdd_86_7 failed\n",
      "25/04/23 17:46:00 WARN MemoryStore: Not enough space to cache rdd_96_7 in memory! (computed 16.9 MiB so far)\n",
      "25/04/23 17:46:00 WARN BlockManager: Block rdd_96_7 could not be removed as it was not found on disk or in memory\n",
      "25/04/23 17:46:00 WARN BlockManager: Putting block rdd_96_7 failed\n",
      "[Stage 260:==========>     (8 + 4) / 12][Stage 287:=============>   (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+\n",
      "| id|         pagerank|\n",
      "+---+-----------------+\n",
      "| 21|6384.880075830329|\n",
      "|  6|5652.756230220395|\n",
      "|  7|5362.062282094499|\n",
      "| 11|4306.846780175733|\n",
      "|  1|3595.660142415134|\n",
      "+---+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# c. Calculate PageRank for each of the nodes and output the top 5 nodes with the highest PageRank values. You are free to define any suitable parameters.\n",
    "pr = g.pageRank(resetProbability=0.15, maxIter=10)\n",
    "pr.vertices.orderBy(\"pagerank\", ascending=False).select(\"id\", \"pagerank\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adaebaee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/23 17:46:06 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/23 17:46:06 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|component| count|\n",
      "+---------+------+\n",
      "|        1|306995|\n",
      "|      278|   344|\n",
      "|     4203|   253|\n",
      "|      448|   131|\n",
      "|     9477|   120|\n",
      "+---------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# d. Run the connected components algorithm on it and find the top 5 components with the largest number of nodes.\n",
    "spark.sparkContext.setCheckpointDir(\"/tmp/graphframes-checkpoints\")\n",
    "cc = g.connectedComponents(algorithm=\"graphframes\", checkpointInterval=2, broadcastThreshold=10000000)\n",
    "cc.groupBy(\"component\").count().orderBy(\"count\", ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "628b97fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/23 17:47:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/23 17:47:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/23 17:47:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/23 17:47:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/23 17:47:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/23 17:47:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/23 17:47:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/23 17:47:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/23 17:47:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/23 17:47:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/23 17:47:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/23 17:47:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/23 17:47:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/23 17:47:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/23 17:47:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/23 17:47:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/23 17:47:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/04/23 17:47:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "[Stage 708:============>                                            (2 + 7) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id| count|\n",
      "+---+------+\n",
      "| 21|238877|\n",
      "|  7|199434|\n",
      "| 11|112001|\n",
      "|  6|100261|\n",
      "| 38| 99971|\n",
      "+---+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# e. Run the triangle counts algorithm on each of the vertices and output the top 5 vertices with the largest triangle count. In case of ties, you can randomly select the top 5 vertices.\n",
    "triangles = g.triangleCount()\n",
    "triangles.orderBy(\"count\", ascending=False).select(\"id\", \"count\").show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
